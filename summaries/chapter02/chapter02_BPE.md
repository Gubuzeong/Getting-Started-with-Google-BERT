# Chapter 2.5 하위 단어 토큰화 알고리즘



## Tokenization

문장을 token sequence로 나누는 과정. 토큰화를 수행하는 프로그램을 토크나이저라고 한다.



#### 단어 단위 토큰화

- 단어(어절) 단위로 토큰화를 수행. 가장 단순한 방법으로는 공백으로 구분하는 방법이 있다
- 어휘 집합(vocabulary)의 크기가 매우 커질 수 있음
  - "먹었다, 먹었는데, 먹었고..." 등 표현이 살짝만 바뀐 어휘를 집합에 포함시켜야 되기 때문
- 학습된 토크나이저를 사용하여 집합의 크기가 커지는 것을 완화할 수 있음
  - "먹었다, 먹었는데, 먹었고..." -> {"먹었", "다", "는데", "고"}



#### 문자 단위 토큰화

- 문자 단위로 토큰화를 수행
  - 한글로 표현할 수 있는 글자는 모두 1만 1,172개이므로 알파벳, 숫자, 기호 등을 고려해도 15,000개 이내로 토큰화 수행 가능
- 모든 문자를 vocab에 추가하므로 미등록 토큰 문제에서 자유롭다

- 각 문자 토큰이 의미있는 단위가 되기 어렵다.
  - "어제" -> {"어", "제"} 로 토큰화가 수행되는데, "어"가 "어제"의 "어"인지, 어미 "어" 인지 알 수 없음

- 문자 단위로 나누기 때문에 토큰 시퀀스의 길이가 길어진다



#### 서브워드 단위 토큰화

- 단어와 문자 단위 토큰화의 중간에 있는 형태
  - 어휘 집합 크기가 지나치게 커지지 않으면서
  - 미등록 토큰 문제를 피하고
  - 분석된 토큰 시퀀스가 너무 길어지지 않게 한다

- 대표적으로 BPE 알고리즘이 이에 해당





## BPE: Byte Pair Encoding

1994년 제안된 정보 압축 알고리즘으로, 가장 많이 등장한 문자열을 병합해서 데이터를 압축하는 기법이다. 최근 자연어 처리 모델에 널리 쓰인다. 알고리즘의 순서는 간단하게 아래와 같이 요약할 수 있다.

​	(1) 문자열을 글자 단위로 분리한다.

​	(2) 모든 글자들을 vocab에 등록한다.

​	(3) 모든 글자의 쌍(pair)이 등장하는 빈도를 측정한다.

​	(4) 가장 빈도가 높은 쌍을 vocab에 등록하며, 모든 문자열들에서 해당 쌍을 하나의 문자열로 병합한다.

​	(5) 처음에 정해놓은 횟수가 될 때까지 (3)과 (4)를 반복한다.

이렇게 토큰화된 vocab에 없는 단어가 있다면 unknown 토큰인 \<UNK>로 대체한다.



아래와 같은 데이터가 있다고 가정하자.

> aaabdaaabac

BPE는 데이터에 등장한 글자 (a, b, c, d)를 초기 사전으로 구성하며, 사전에 등록된 글자들의 모든 쌍의 빈도를 확인하고, 가장 많이 나타난 쌍을 병합하여 사전에 추가한다. 가장 많이 나타난 쌍은 (aa) 이므로 이를 병합한다. 이를 Z로 표현한다.

> **Z**abd**Z**abac

이 문자열을 한번 더 압축한다면, ab가 많이 나타났으니 (ab)를 병합할 수 있다. 이를 Y라고 표현한다.

- 물론 (Za)를 병합할 수 있지만, (ab)와 같은 빈도수 2를 가지고 있으므로, 알파벳 순으로 앞선 (ab)를 병합하기로 선택했다. 이는 사용자가 조절할 수 있다.

> Z**Y**dZ**Y**ac

(ZY)가 가장 많은 빈도수를 가지고 있으므로 문자열을 한번 더 병합할 수 있다. 이를 X로 표현한다.

> **X**d**X**ac



**BPE 수행 전**

- 사전을 통해 4개의 문자를 표현할 수 있다. (a, b, c, d)

- 문자열의 데이터 길이는 11이다. (aaabdaaabac)

**BPE 수행 후**

- 사전을 통해 7개의 문자를 표현할 수 있다. (a, b, c, d, Z=(aa), Y=(ab), X=(ZY))

- 문자열의 데이터 길이는 5이다. (XdXac)



BPE는 사전의 크기를 지나치게 늘리지 않으면서 데이터 길이를 효율적으로 압축할 수 있도록 한다. 또한 BPE 기반 토큰화 기법은 분석 대상 언어에 대한 지식이 필요 없다. 말뭉치에서 자주 나타나는 문자열을 토큰으로 분석하기 때문이다.





#### WordPiece

- BPE와 유사지만, 토큰 쌍의 빈도가 아닌 가능도(likelihood)를 사용한다.

$$
{{\#ab} \over n} \over {{\#a \over n} \times {\#b \over n}}
$$

- 위 수식의 값이 커지려면 a, b 가 서로 독립임을 가정했을 때, 둘이 자주 동시에 등장해야 한다. (둘이 자주 등장하지만 동시에 등장하지 않으면 likelihood값이 감소한다)





## Reference

- 구글 BERT의 정석 [book]
- BERT와 GPT로 배우는 자연어 처리 [book]

## BERTë€ ë¬´ì—‡ì¸ê°€

BERT(Bidirectional Encoder Representation from Transformer) : Googleì—ì„œ ë§Œë“  ë¬¸ë§¥ì„ ê³ ë ¤í•œ Transformer ê¸°ë°˜ ê³ ì„±ëŠ¥ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸.

### ì„ë² ë”© ëª¨ë¸ì´ ë¬¸ë§¥ì„ ê³ ë ¤í•  ë•Œì˜ ì¥ì 

**ğŸ‘‰ğŸ» ë‹¤ì˜ì–´âˆ™ë™ìŒì´ì˜ì–´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.**

> A: He got bit by Python(íŒŒì´ì¬ì´ ê·¸ë¥¼ ë¬¼ì—ˆë‹¤).
> 
> B: Python is my favorite programming language(ë‚´ê°€ ì œì¼ ì¢‹ì•„í•˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ” íŒŒì´ì¬ì´ë‹¤).

-   **Word2Vec**: `ì •ì  ì„ë² ë”©`, Aì—ì„œì˜ 'Python' ì„ë² ë”© == Bì—ì„œì˜ 'Python' ì„ë² ë”©
-   **BERT**: `ë™ì  ì„ë² ë”©`, íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê¸°ë°˜ì´ë¯€ë¡œ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ë“¤ê³¼ ì—°ê²°ì‹œì¼œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤. Aì—ì„œëŠ” 'Python-bit'ê³¼ì˜ ê´€ê³„ì— ì£¼ëª©, Bì—ì„œëŠ” 'Python-programming' ê´€ê³„ì— ì£¼ëª©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì„ë² ë”© ê°’ì„ ê°–ëŠ”ë‹¤.

## BERTì˜ ë™ì‘ ë°©ì‹

íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì—ì„œ **ì¸ì½”ë”**ë§Œ ì‚¬ìš©í•œë‹¤. ì¸ì½”ë”ëŠ” ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ë¬¸ë§¥ì„ ê³ ë ¤í•´ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ì—¬ ê° ë‹¨ì–´ì˜ **ë¬¸ë§¥ í‘œí˜„**ì„ ì¶œë ¥í–ˆë‹¤.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FemKM2h%2Fbtrq69SB0j8%2FBC4oGPlf1grsVYSuimVJnk%2Fimg.png)

ë¬¸ì¥ì´ ì¸ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´ ì¸ì½”ë”ëŠ” ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë‹¨ì–´ë¼ë¦¬ ëª¨ë‘ ì—°ê²°í•˜ì—¬ ê´€ê³„ì™€ ë¬¸ë§¥ì„ íŒŒì•…í•´ ë¬¸ì¥ ê° ë‹¨ì–´ì˜ ë¬¸ë§¥ í¬í˜„ì„ ì¶œë ¥í•œë‹¤. ì¸ì½”ë”ëŠ” ì—¬ëŸ¬ ê°œ ìŒ“ì„ ìˆ˜ ìˆìœ¼ë©°, ê° ë‹¨ì–´ í† í°ì˜ í‘œí˜„ í¬ê¸°ëŠ” ì¸ì½”ë” ë ˆì´ì–´ ì¶œë ¥ì˜ ì°¨ì›ì´ë‹¤.

## BERTì˜ êµ¬ì¡°

BERTëŠ” í¬ê¸°ì— ë”°ë¼ ì•„ë˜ì˜ ë‘ ëª¨ë¸ë¡œ ë‚˜ë‰œë‹¤.

-   BERT-base: OpenAI GPTì™€ ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§. GPTì™€ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ ì„¤ê³„ë¨
-   BERT-large: BERTì˜ ìµœëŒ€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FMLve3%2FbtrrbTH1MZ5%2FZqfEvXKErT6w4F944AFohk%2Fimg.png)


### GLUE Results

![ê·¸ë¦¼5. GLUE results](https://mino-park7.github.io/images/2019/02/%EA%B7%B8%EB%A6%BC5-glue-results.png)

-   ëª¨ë“  taskì— ëŒ€í•´ SOTA ë‹¬ì„±
-   BERT-largeê°€ ì¼ë°˜ì ìœ¼ë¡œ BERT-baseë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŒ
-   **ì‚¬ì „í•™ìŠµ** ë•ë¶„ì— ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ì‘ì•„ë„ ëª¨ë¸ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì •í™•ë„ê°€ ìƒìŠ¹

## BERT ì‚¬ì „ í•™ìŠµ

BERTì— ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê¸° ì „ì— ì„¸ ê°€ì§€ ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ ì…ë ¥ ë°ì´í„°ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.

### ì…ë ¥ ì„ë² ë”©

![ê·¸ë¦¼2. bert input representation (ì¶œì²˜: BERT ë…¼ë¬¸)](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)

-   í† í° ì„ë² ë”©(token embedding)
-   ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©(segement embedding)
-   ìœ„ì¹˜ ì„ë² ë”©(position embedding)

#### í† í° ì„ë² ë”©

-   ë¬¸ì¥ ìŒ(Sentence pair)ì€ í•©ì³ì ¸ì„œ ë‹¨ì¼ ì‹œí€€ìŠ¤ë¡œ ì…ë ¥ë˜ë©°, ì…ë ¥ ë‚´ì˜ ìŒì€ í•œ ê°œ í˜¹ì€ ë‘ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆì„ ìˆ˜ ìˆë‹¤.
    -   ì˜ˆì‹œ: QA Task - \[Question, Paragraph]
        -   Q: What is your favorite programming language?
        -   A: My favorite programming language is Python.
-   Sentenceì˜ ì‹œì‘ ë¶€ë¶„ì— `[CLS]`ë¼ëŠ” í† í°ì„ ì¶”ê°€í•œë‹¤.
    -   ë¶„ë¥˜ ë¬¸ì œë¥¼ í’€ ë•Œë§Œ ì‚¬ìš©ë˜ì§€ë§Œ ë‹¤ë¥¸ ë¬¸ì œë¥¼ í’€ë”ë¼ë„ ë¬´ì¡°ê±´ ì¶”ê°€í•´ì•¼ í•œë‹¤.
-   Sentence ë‚´ ëª¨ë“  ë¬¸ì¥ì˜ ëì— `[SEP]`ë¼ëŠ” ìƒˆ í† í°ì„ ì¶”ê°€í•œë‹¤.
-   í† í° ì„ë² ë”©ì„ ê±°ì¹œ í›„ì˜ í˜•íƒœ: `tokens = [[CLS], My, favorite, ... , [SEP], It's, ..., use, [SEP]]`

#### ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €

BERTì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í¬ë‚˜ì´ì €ëŠ” ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ìª¼ê°œ í† í°í™”í•˜ëŠ” ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì € ê¸°ë°˜ì˜ **ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €**ë‹¤. ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ìª¼ê°œ í† í°í™”í–ˆì„ ë•Œì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

-   **OOV(out-of-vocabulary)ì˜ ì²˜ë¦¬ê°€ ì‰¬ì›Œì§„ë‹¤**. ë‹¨ì–´ê°€ ì–´íœ˜ ì‚¬ì „ì— ì—†ìœ¼ë©´ ê³„ì†í•´ì„œ í•˜ìœ„ ë‹¨ì–´ë¡œ ìª¼ê°œê°€ë©° ê°œë³„ ë¬¸ìì— ë„ë‹¬í•  ë•Œê¹Œì§€ í™•ì¸í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
-   **ê³„ì‚° ë¹„ìš©ì„ ë¹„êµì  ì‘ê²Œ ìœ ì§€í•  ìˆ˜ ìˆë‹¤**. í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì˜ í¬ê¸°ëŠ” Vocabularyì˜ í¬ê¸°ì— ì˜í–¥ì„ ë°›ê³  ì´ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ê³„ì‚°ë¹„ìš©ì´ ì¦ê°€í•˜ëŠ”ë°, í•˜ìœ„ ë‹¨ì–´ë¡œ ìª¼ê°œ í† í°í™”í•˜ë©´ Vocabularyì˜ í¬ê¸°ë¥¼ ì‘ê²Œ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.
-   ê³ ìœ ì˜ ì•Œê³ ë¦¬ì¦˜ ë•ë¶„ì— **ì–¸ì–´ì— ìƒê´€ì—†ì´ ë²”ìš©ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤.**

#### ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ê°€?

ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ê°€ ì²˜ìŒìœ¼ë¡œ ë“±ì¥í•œ [ë…¼ë¬¸](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)ì˜ ì„¤ëª…ì— ë”°ë¥´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ë”°ë¼ ë™ì‘í•œë‹¤.

1.  ê¸°ë³¸ ê¸€ìë“¤(ì•ŒíŒŒë²³ ë“±)ìœ¼ë¡œ ë‹¨ì–´ ìœ ë‹› ì¸ë²¤í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
2.  1ì—ì„œ ìƒì„±ëœ ë‹¨ì–´ ì¸ë²¤í† ë¦¬ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ë¡œ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“ ë‹¤.
3.  í˜„ì¬ì˜ ë‹¨ì–´ ì¸ë²¤í† ë¦¬ì—ì„œ ë‘ ê°œì˜ ìœ ë‹›ì„ ê²°í•©í•´ ìƒˆë¡œìš´ ë‹¨ì–´ ìœ ë‹›ì„ ìƒì„±í•œë‹¤. ì´ ë•Œ ì„ íƒë˜ëŠ” ë‘ ë‹¨ì–´ëŠ” ê²°í•©í–ˆì„ ë•Œì˜ **ê°€ëŠ¥ë„(likelihood)** ìƒìŠ¹í­ì´ ê°€ì¥ í° ë‹¨ì–´ë“¤ì´ë‹¤. ğŸ‘‰ğŸ» ëª¨ìˆ˜ ì¶”ì •ì„ ìœ„í•´ **ML(maximize likelihood)** ì‚¬ìš©
4.  ë¯¸ë¦¬ ì •í•´ë‘” ë‹¨ì–´ ì¸ë²¤í† ë¦¬ í¬ê¸° í•œë„ì— ë„ë‹¬í•˜ê±°ë‚˜ ê°€ëŠ¥ë„ì˜ ì¦ê°€í­ì´ íŠ¹ì • ì„ê³„ì  ì•„ë˜ë¡œ ë‚´ë ¤ê°ˆ ë•Œê¹Œì§€ `Goto 2` -> ë°˜ë³µ

> ê°€ëŠ¥ë„(likelihood): ì–´ë–¤ ê°’ì´ ê´€ì¸¡ë˜ì—ˆì„ ë•Œ, ì´ê²ƒì´ ì–´ë–¤ í™•ë¥  ë¶„í¬ì—ì„œ ì™”ì„ì§€ì— ëŒ€í•œ í™•ë¥ 

#### ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©

-   ê°™ì€ ë¬¸ì¥ ë‚´ì˜ ì„œë¡œ ë‹¤ë¥¸ ë‘ ì‘ì€ ë¬¸ì¥ì„ êµ¬ë³„í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.
-   ë‘ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ `[SEP]` í† í° ì‚¬ìš©ì— ì¶”ê°€ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©ì„ ì‚¬ìš©í•´ì„œ ì•ì˜ ë¬¸ì¥ì—ëŠ” `sentence A embedding`, ë’¤ì˜ ë¬¸ì¥ì—ëŠ” `sentence B embedding`ì„ ë”í•´ì¤€ë‹¤.
-   ë¬¸ì¥ì´ í•˜ë‚˜ë¼ë©´ `sentence A embedding` ë§Œì„ ì‚¬ìš©í•œë‹¤.

#### ìœ„ì¹˜ ì„ë² ë”©

-   BERTì˜ ë² ì´ìŠ¤ê°€ ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë‹¨ì–´ì˜ ìˆœì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ë”°ë¡œ ì œê³µí•´ ì¤˜ì•¼ í•œë‹¤. ìœ„ì¹˜ ì„ë² ë”©ì„ í†µí•´ ë¬¸ì¥ì˜ ê° í† í°ì— ëŒ€í•œ ìœ„ì¹˜ ì„ë² ë”© ì¶œë ¥ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
-   BERTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë‹¤ë¥´ê²Œ í•™ìŠµì„ í†µí•´ ìœ„ì¹˜ ì •ë³´ë¥¼ ì–»ëŠ” í¬ì§€ì…˜ ì„ë² ë”©ì„ ì‚¬ìš©í•œë‹¤. ë¬¸ì¥ì˜ ê¸¸ì´ë§Œí¼ì˜ í¬ì§€ì…˜ ì„ë² ë”© ë²¡í„°ë¥¼ í•™ìŠµì‹œì¼œ ì‚¬ìš©í•œë‹¤.

### ì‚¬ì „í•™ìŠµì— ëŒ€í•œ ê¸°ì¡´ ë°©ë²•ë¡ 

![ê·¸ë¦¼1. BERT, GPT, ELMo (ì¶œì²˜ : BERT ë…¼ë¬¸)](https://mino-park7.github.io/images/2018/12/%EA%B7%B8%EB%A6%BC1-bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png)

-   ì „í†µì ì¸ ì–¸ì–´ ëª¨ë¸ë§(Language Modeling): **n-gram**, ì•ì˜ N-1ê°œì˜ ë‹¨ì–´ë¡œ ë’¤ì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸
-   í•„ì—°ì ìœ¼ë¡œ ë‹¨ë°©í–¥ì¼ìˆ˜ ë°–ì— ì—†ê³ , BiLMì„ ì‚¬ìš©í•˜ëŠ” ELMoë”ë¼ë„ ìˆœë°©í–¥, ìˆœë°©í–¥ì˜ ì–¸ì–´ ëª¨ë¸ì„ ë‘˜ ë‹¤ í•™ìŠµí•´ í™œìš©í•˜ì§€ë§Œ, ë‹¨ë°©í–¥ ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì„ concatí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ì •ë„ì´ë¯€ë¡œ ì œí•œì ì¸ ì–‘ë°©í–¥ì„±ì„ ê°€ì§

### BERT ì‚¬ì „í•™ìŠµì— ì‚¬ìš©ëœ ìƒˆë¡œìš´ ë°©ë²•ë¡ 

-   Masked Language Model(MLM)
-   Next Sentence Prediction(NSP)

#### ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§ Masked Language Model

BERTëŠ” ì–‘ë°©í–¥ì„±ì„ MLMì„ í†µí•´ êµ¬í˜„í–ˆë‹¤. MLMì€ ìë™ ì¸ì½”ë”© ì–¸ì–´ ëª¨ë¸ë¡œ, ì˜ˆì¸¡ì„ ìœ„í•´ ë¬¸ì¥ì„ **ì–‘ë°©í–¥**ìœ¼ë¡œ ì½ëŠ”ë‹¤. ì „ì²´ ë‹¨ì–´ì˜ 15%ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ê³ , ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ë©° ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ëª¨ë¸ì€ ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ì¥ì„ ì½ê³  ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë ¤ ì‹œë„í•œë‹¤. `[MASK]` í† í°ì€ ì‚¬ì „í•™ìŠµì—ì„œë§Œ ì‚¬ìš©ë˜ë©°, íŒŒì¸ íŠœë‹ ì‹œì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.

15%ì˜ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•  ë•Œ `80-10-10%` ê·œì¹™ì„ ì ìš©í•œë‹¤.

-   80%: í† í°ì„ `[MASK]`ë¡œ ë°”ê¾¼ë‹¤.
    -   ì˜ˆ) `So we could call it even` -> `So we could [MASK] it even`
-   10%: í† í°ì„ ì„ì˜ì˜ í† í°(ë‹¨ì–´)ë¡œ ë°”ê¾¼ë‹¤.
    -   ì˜ˆ) `So we could call it even` -> `So we could pizza it even`
-   10%: ì–´ë– í•œ ë³€ê²½ë„ í•˜ì§€ ì•ŠëŠ”ë‹¤. BERTëŠ” ì´ ë‹¨ì–´ê°€ ë³€ê²½ëœ ë‹¨ì–´ì¸ì§€ ì›ë˜ ë‹¨ì–´ì¸ì§€ ëª¨ë¥´ë¯€ë¡œ BERTê°€ ì›ë˜ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì˜ˆì¸¡í•˜ë„ë¡ í•œë‹¤.

**ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹ Whole Word Masking**

ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹ì€ ë‹¨ì–´ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ëŠ” ê³¼ì •ì—ì„œ í•˜ìœ„ ë‹¨ì–´ê°€ ì„ íƒë˜ì—ˆì„ ë•Œ, í•´ë‹¹ í•˜ìœ„ ë‹¨ì–´ì™€ ê´€ë ¨ëœ ëª¨ë“  ë‹¨ì–´ë¥¼ ë§ˆìŠ¤í‚¹í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ëŠ” WWMì˜ ë™ì‘ ì˜ˆì‹œë‹¤.

```
#1
tokens = [[CLS], let, us, start, pre, ##train, ##ing, the, model, [SEP]]
#2 - ##trainì´ ë§ˆìŠ¤í‚¹ë¨
tokens = [[CLS], let, us, start, pre, [MASK], ##ing, the, model, [SEP]]
#3 - ##trainê³¼ ê´€ë ¨ëœ preì™€ ##ingë„ ë§ˆìŠ¤í‚¹ë¨
tokens = [[CLS], let, us, start, [MASK], [MASK], [MASK], the, model, [SEP]]
```

ë§ˆìŠ¤í‚¹ëœ í•˜ìœ„ ë‹¨ì–´(ì˜ˆì‹œì—ì„œëŠ” `##train`)ì™€ ê´€ë ¨ëœ ëª¨ë“  ë‹¨ì–´ë¥¼ ë§ˆìŠ¤í‚¹í•˜ëŠ” ë™ì•ˆ ë§ˆìŠ¤í¬ ë¹„ìœ¨(15%)ì„ ì´ˆê³¼í•˜ë©´ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ë§ˆìŠ¤í‚¹ì„ ë¬´ì‹œí•œë‹¤.

**ë§ˆìŠ¤í¬ëœ í† í° ì˜ˆì¸¡**

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FD174B%2Fbtrq8JFSKdF%2FseB75GL8Ustkoq4v9JtzrK%2Fimg.png)

í† í°í™”ì™€ WWMì„ ê±°ì¹œ í›„ì— ì…ë ¥ í† í°ì„ í† í°, ì„¸ê·¸ë¨¼íŠ¸, ìœ„ì¹˜ ì„ë² ë”© ë ˆì´ì–´ì— ì…ë ¥í•´ ì…ë ¥ ì„ë² ë”©ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ ì…ë ¥ ì„ë² ë”©ì„ BERTì— ì œê³µí•˜ë©´ BERTëŠ” ê° í† í°ì˜ í‘œí˜„ $R$ì„ ì¶œë ¥í•œë‹¤. ê° ë‹¨ì–´ë“¤ì˜ í‘œí˜„ë“¤($R$) ì¤‘ ë§ˆìŠ¤í¬ëœ í† í°ì˜ í‘œí˜„ ë²¡í„° $R_{MASK}$ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤ í™œì„±í™”ë¥¼ í†µí•´ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ì— ì…ë ¥í•˜ë©´, ê·¸ ì¶œë ¥ìœ¼ë¡œ ê° ë‹¨ì–´ê°€ \[MASK\] ìë¦¬ì— ìˆì–´ì•¼ í•  ë‹¨ì–´ì¼ í™•ë¥ ì„ ë°˜í™˜í•œë‹¤.

#### ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ Next Sentence Prediction

QA, Natural Language Inference(NLI)ì²˜ëŸ¼ NLP íƒœìŠ¤í¬ ì¤‘ì—ì„  ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê²ƒë“¤ì´ ìˆëŠ”ë°, ì´ê²ƒì€ ì „í†µì ì¸ ì–¸ì–´ ëª¨ë¸ë§(n-gram)ì—ì„œ í•™ìŠµë  ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì´ë‹¤. ë”°ë¼ì„œ BERTëŠ” NSPë¼ê³  ë¶ˆë¦¬ëŠ” ë‘ ë¬¸ì¥ì„ ì…ë ¥í•˜ê³  ë‘ë²ˆì§¸ ë¬¸ì¥ì´ ì²«ë²ˆì§¸ ë¬¸ì¥ì˜ ë‹¤ìŒ ë¬¸ì¥ì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•œë‹¤.

-   í•™ìŠµì„ ìœ„í•´ 50%ëŠ” ì‹¤ì œë¡œ ì´ì–´ì§€ëŠ” ë‘ ë¬¸ì¥ì„ ë„£ëŠ”ë‹¤.
-   ë‚˜ë¨¸ì§€ 50%ì€ ëœë¤ìœ¼ë¡œ ì¶”ì¶œëœ ë‘ ë¬¸ì¥ì„ ë„£ëŠ”ë‹¤.

ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë ˆì´ë¸”ë§ ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. ì„œë¡œ ì´ì–´ì§€ëŠ” ë¬¸ì¥ ìŒì—ëŠ” `IsNext` ë ˆì´ë¸”ì„ ë¶™ì´ê³ , ì„œë¡œ ì´ì–´ì§€ì§€ ì•ŠëŠ” ë¬¸ì¥ ìŒì—ëŠ” `NotNext` ë ˆì´ë¸”ì„ ë¶™ì—¬ ë‘ ë¬¸ì¥ì´ ì´ì–´ì§€ì§€ ì•ŠìŒì„ í‘œì‹œí•œë‹¤.

-   `[CLS] He got [MASK] by Python [SEP] So now he's bleeding LABEL = IsNext`
-   `[CLS] He got [MASK] by Python [SEP] Let's go out and get some pizza LABEL = NotNext`

ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” `[CLS]` í† í°ì˜ í‘œí˜„ê°’ì„ ì‚¬ìš©í•œë‹¤. `[CLS]`ëŠ” ë¬¸ì¥ ë‚´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ self-attentionì„ í†µí•´ ëª¨ë“  í† í°ì˜ ì§‘ê³„ í‘œí˜„ì„ ë‹´ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤. ([ì°¸ê³ ](https://stackoverflow.com/questions/62705268/why-bert-transformer-uses-cls-token-for-classification-instead-of-average-over)) `[CLS]` í† í°ì— classification layerë¥¼ ë¶™ì´ê³  softmaxë¥¼ í†µí•´ ê° ë ˆì´ë¸”ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•œë‹¤.
![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F2PYC6%2FbtrrcHGViek%2FHviweSdG42hgbtp6uvOkx0%2Fimg.png)

### ì‚¬ì „ í•™ìŠµ ì ˆì°¨

1.  ë§ë­‰ì¹˜ì—ì„œ ë‘ ë¬¸ì¥ A, Bë¥¼ ìƒ˜í”Œë§í•œë‹¤.
    -   Aì™€ Bì˜ ì´ í† í° ìˆ˜ì˜ í•©ì€ 512ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì•„ì•¼ í•œë‹¤.
    -   ì „ì²´ì˜ 50%ì€ B ë¬¸ì¥ì´ A ë¬¸ì¥ê³¼ ì´ì–´ì§€ëŠ” ë¬¸ì¥(`IsNext`)ì´ ë˜ë„ë¡ ìƒ˜í”Œë§í•˜ê³ , ë‚˜ë¨¸ì§€ 50%ì€ B ë¬¸ì¥ì´ A ë¬¸ì¥ì˜ í›„ì† ë¬¸ì¥ì´ ì•„ë‹Œ ê²ƒ(`NotNext`)ìœ¼ë¡œ ìƒ˜í”Œë§í•œë‹¤.
2.  ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°í™”í•˜ê³ , í† í° ì„ë² ë”©-ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©-ìœ„ì¹˜ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê±°ì¹œë‹¤.
    -   ì‹œì‘ ë¶€ë¶„ì— `[CLS]` í† í°ì„, ë¬¸ì¥ ëì— `[SEP]` í† í°ì„ ì¶”ê°€í•œë‹¤.
    -   `80-10-10%` ê·œì¹™ì— ë”°ë¼ í† í°ì˜ 15%ë¥¼ ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹í•œë‹¤.
3.  BERTì— í† í°ì„ ì…ë ¥í•˜ê³ , MLMê³¼ NSP íƒœìŠ¤í¬ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.
    -   ì›œì—… ìŠ¤í…(= 1ë§Œ): ì´ˆê¸° 1ë§Œ ìŠ¤í…ì€ í•™ìŠµë¥ ì´ 0ì—ì„œ 1e - 4ë¡œ ì„ í˜• ì¦ê°€, 1ë§Œ ìŠ¤í… ì´í›„ ì„ í˜• ê°ì†Œ
    -   ë“œë¡­ì•„ì›ƒ(0.1) ì‚¬ìš©
    -   **GeLU** í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©: ìŒìˆ˜ì— ëŒ€í•´ì„œë„ ë¯¸ë¶„ì´ ê°€ëŠ¥í•´ ì•½ê°„ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŒ

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqAHBZ%2Fbtrq9XqmTm1%2FowmFV6JfVGKHEvlc1iju51%2Fimg.png)

### ì‚¬ì „ í•™ìŠµì˜ íš¨ê³¼

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbyikya%2FbtrrcN1nSMg%2Frz2de6QueHCWKP41BV0ZCk%2Fimg.png)

> **No NSP**: MLM ì‚¬ìš© / NSP ë¯¸ì‚¬ìš©
> 
> **LTR & No NSP**: MLM ëŒ€ì‹  Left-to-Right ì‚¬ìš© / NLP ë¯¸ì‚¬ìš©

-   NSP íƒœìŠ¤í¬ë¥¼ ì§„í–‰í•˜ì§€ ì•Šìœ¼ë©´ ìì—°ì–´ ì¶”ë¡  íƒœìŠ¤í¬(QNLI, MNLI)ì™€ QA íƒœìŠ¤í¬(SQuAD)ì—ì„œ í° ì„±ëŠ¥ í•˜ë½ì´ ìˆìŒ
-   MLM ëŒ€ì‹  LTRì´ë‚˜ BiLSTMì„ ì‚¬ìš©í–ˆì„ ë•Œ MRPCì™€ SQuADì—ì„œì˜ ì„±ëŠ¥ì´ í¬ê²Œ í•˜ë½í•¨. MLMì´ LTRê³¼ BiLSTMë³´ë‹¤ í›¨ì”¬ ê¹Šì€ ì–‘ë°©í–¥ì„±ì„ ëˆë‹¤.

## ì°¸ê³ ìë£Œ

-   [BERT ë…¼ë¬¸](https://arxiv.org/abs/1810.04805)
-   [BERT ë…¼ë¬¸ì •ë¦¬ âˆ™ Minho Park](https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w#bert%EC%9D%98-pre-training-%EB%B0%A9%EB%B2%95%EB%A1%A0)
-   [Wikidocs: ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/115055)
-   [WordPiece: Subword-based tokenization algorithm](https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7)

# Chapter 7. 다른 언어에 BERT 적용하기

BERT를 영어가 아닌 다른 언어에도 적용할 수 있을까? 아래 모델을 통해서 BERT를 통해 다국어 표현을 어떻게 얻을 수 있는지 알아보자.

- M-BERT(multilingual-BERT)
- XLM(cross-lingual language model)
- XLM-R(XLM-RoBERTa)



## M-BERT 이해하기

**BERT**

- Datasets: 영어 위키피디아, 토론토 책 말뭉치
- PreTrain Task: MLM, NSP

**M-BERT**

- Datasets: 104개 언어 위키피디아 
  - 각 언어들간의 비중이 다르다 -> overfitting을 방지하기 위해 언어별 under/over sampling 적용
  - 104개 언어, 11만개의 워드피스(BPE와 유사하지만 likelihood 사용하는 인코딩)
- PreTrain Task: MLM, NSP



M-BERT는 교차 언어를 고려하지 않고도 다른 언어들의 표현을 이해할 수 있다.



### NLI task 평가로 알아보는 M-BERT 평가하기

- 두 문장(가설, 전제)을 바탕으로, 가설이 주어진 전제에 따라 진실/거짓/중립 여부를 판단하는 것
- 일반적으로 SNLI(Stanford natural language inference) datasets 사용
- 다국어 NLI 평가를 위해서는 교차 언어 자연어 추론(XNLI: cross-linugal NLI)사용
  - MultiNLI 말뭉치에서 약 43만개의 영어 문장 쌍을 사용
  - 평가셋을 위해 7,500개의 문장 쌍을 사용하며, 이를 15개의 서로 다른 언어로 번역하여 11만개의 문장 쌍을 사용



다양한 설정에서 NLI task를 수행해 M-BERT를 평가한다.

#### 제로샷(zero-shot)

- Fine-tuning: 영어 학습셋
- Validation: 모든 언어 테스트셋
- 영어로 학습하고 모든 언어로 평가
- M-BERT의 교차 언어 능력을 이해하는데 도움



#### 번역-테스트(Translate-Test)

- Fine-tuning: 영어 학습셋
- Validation: 영어로 번역된 테스트셋
- 영어로 학습하고 영어로 평가



#### 번역-학습(Translate-Train)

- Fine-tuning: 영어에서 다른 언어로 번역된 학습셋
- Validation: 모든 언어 테스트셋
- 다른 언어로 학습하고 모든 언어로 평가



#### 번역-학습-모두(Translate-Train-All)

- Fine-tuning: 영어에서 다른 **모든 언어**로 번역된 학습셋
- Validation: 모든 언어 테스트셋
- 모든 언어로 학습하고 모든 언어로 평가



| 모델         | 설정            | 영어 | 중국어 | 스페인어 | 독일어 | 아랍어 | 우루드어 |
| ------------ | --------------- | ---- | ------ | -------- | ------ | ------ | -------- |
| BERT-cased   | Translate-Train | 81.9 | 76.6   | 77.8     | 75.9   | 70.7   | 61.6     |
| BERT-uncased | Translate-Train | 81.4 | 74.2   | 77.3     | 75.2   | 70.5   | 61.7     |
| BERT-uncased | Translate-Test  | 81.4 | 70.1   | 74.9     | 74.4   | 70.4   | 62.1     |
| BERT-uncased | Zero-shot       | 81.4 | 74.3   | 74.3     | 70.3   | 60.1   | 58.3     |

> M-BERT 모델은 제로샷을 포함해 모든 조건에서 좋은 성능을 보여준다.



### M-BERT는 다국어 표현이 어떻게 가능한가??

다른 언어들을 다 섞어서 학습한 것 뿐인데 어떻게 다국어 표현이 가능한 건지?



#### 어휘 중복 효과?

- 언어 간에 중복되는 어휘 때문??

- overlap으로 파인튜닝 언어/평가 언어 간에 겹치는 워드피스 토큰 계산
  $$
  \left| \dfrac{E_{train}\cap E_{eva1}}{E_{train}\cup E_{era1}}\right|
  $$

- 16개의 모든 언어 쌍에 대해 제로샷 F1 score를 계산한 결과 어휘 중복(overlap)이 적을 때에도 제로샷 F1이 높았음

> zeroshot 지식 전이는 어휘 중복에 영향을 받지 않는다
>
> M-BERT가 다른 언어와의 관계성을 고려해 일반화를 잘 한다 = 단순한 어휘 암기가 아니라 다국어 표현을 더 깊게 학습한다



#### 스크립트에 대한 일반화?

- 서로 다른 스크립트를 따르는 언어들 사이에서도 일반화가 잘 될까?

> 일부 언어 쌍의 스크립트에서는 일반화가 잘 되지만, 모든 언어에서 적용되지는 않는다.



#### 유형학적 특징에 대한 일반화?

- 주어, 동사, 목적어 등의 단어 순서가 다른 언어들 사이에서도 일반화가 잘 될까?

> M-BERT의 zeroshot 전이는 단어 순서가 동일한 언어에서 더 잘 작동한다.
>
> M-BERT의 일반화 능력은 언어 간의 유형학적 유사성에 따라 달라진다 = M-BERT가 체계적인 변환을 학습하는 것은 아님



#### 언어 유사성의 효과?

- WALS: 문법, 어휘 및 음운 속성과 같은 언어의 구조적 속성을 포함하는 대규모 데이터베이스
- WALS 공통 특징 수가 많을 수록 제로샷 정확도가 높은 경향

> M-BERT는 유사한 언어 구조를 공유하는 언어 사이에서 더 잘 일반화된다



#### 코드 스위칭과 음차의 효과?

**코드 스위칭**

- 다른 언어를 혼합하거나 교대로 사용하는 것
- ex) Korean은 굉장히 kind하고 polite한 것 같아.

**음차**

- 발음하는 그대로 쓴거
- ex) 코리안은 굉장히 카인드하고 폴라이트한 것 같아.



M-BERT에서 코드 스위칭과 음차를 어떻게 다룰까?

- 코드 스위칭된 힌디어/영어 UD 말뭉치를 사용
- 음차: 힌디어 텍스트는 라틴 문자로 작성된다.
- 코드 스위칭: 라틴어의 힌디어 텍스트가 데바나가리어 스크립트로 변환되어 재작성된다.

| 말뭉치                   | 평가셋      | 정확도 |
| ------------------------ | ----------- | ------ |
| 음차                     | 음차        | 85.64  |
| 단일 언어(영어 + 힌디어) | 음차        | 50.41  |
| 코드 스위칭              | 코드 스위칭 | 90.56  |
| 단일 언어(영어 + 힌디어) | 코드 스위칭 | 86.59  |

> M-BERT는 음차 텍스트와 비교해 코드 스위칭 텍스트에서 수행 능력이 좋다



#### 결론

- M-BERT의 일반화 가능성은 어휘 중복에 의존하지 않는다.
- M-BERT의 일반화 가능성은 유형학 및 언어 유사성에 따라 다르다.
- M-BERT는 코드 스위칭 텍스트를 처리할 수 있지만 음차 텍스트는 처리할 수 없다.



## XLM

- 다국어를 목표로 BERT를 사전학습 시킨 모델로 M-BERT보다 다국어 표현 학습을 할 때 성능이 더 뛰어나다

- 단일 언어 / 병렬 데이터셋을 사용해 사전 학습된다
  - 병렬 데이터셋: 언어 쌍의 텍스트(2개의 다른 언어로 된 동일한 텍스트)
- 단일 언어 데이터셋: 위키피디아
- 병렬 데이터셋: 다국어 유엔 말뭉치(MultiUN), 개방형 병렬 말뭉치(OPUS), IIT 봄베이 말뭉치 등의 여러 소스

- BPE 사용

- 언어 임베딩을 사용해서 해당 토큰이 어떤 언어인지 알려준다



### CLM

- 인과 언어 모델링(CLM: Causal Language Modeling)

- 이전 단어셋을 바탕으로 현재 단어의 확률 예측



### MLM

- 마스크 언어 모델링(MLM: Masked Language Modeling)

- BERT에서는 CLS를 위해 두 개의 문장 쌍을 입력했지만, XLM에서는 임의의 문장을 모델에 입력할 수 있다(총 토큰의 길이는 256)
- 단어의 빈도수에 따라 샘플링 빈도가 달라진다(sqrt(1/빈도수)의 다항분포에서 샘플링)



### TLM

- 번역 언어 모델링(TLM: Translation Language Modeling)

- 서로 다른 두 언어/동일한 내용의 텍스트로 구성된 병렬 교차 언어 데이터를 이용
- Masked token을 맞추는 것은 MLM과 동일하지만, 교차 언어 표현을 학습시키려는 의도
  - 마스크된 토큰을 맞추기 위해서 다른 언어의 표현을 이해할 수 있다(교차 언어 표현이 정렬(align)된다)



### XLM 사전 학습

- CLM or MLM
  - 단일 언어 데이터셋
  - 총 256개의 토큰으로 된 임의의 문장
- TLM + MLM
  - TLM의 경우 병렬 데이터셋 사용



### XLM 평가

![img](https://miro.medium.com/max/1400/0*MYR0zGQXyTuupM-Y.png)










# Chapter 9. VideoBERT

## VideoBertë¡œ ì–¸ì–´ ë° ë¹„ë””ì˜¤ í‘œí˜„ í•™ìŠµ

**VideoBERT** ğŸ“„ [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/pdf/1904.01766.pdf)

![text-to-video generation and future forcasting examples](https://d3i71xaburhd42.cloudfront.net/c41a11c0e9b8b92b4faaf97749841170b760760a/2-Figure2-1.png)

- ì˜ìƒê³¼ ì–¸ì–´ì˜ í‘œí˜„ì„ ë™ì‹œì— ë°°ìš°ëŠ” ìµœì´ˆì˜ BERT ëª¨ë¸
- ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±, ë¹„ë””ì˜¤ ìº¡ì…˜, ë¹„ë””ì˜¤ì˜ ë‹¤ìŒ í”„ë ˆì„ ì˜ˆì¸¡ ë“±ê³¼ ê°™ì€ íƒœìŠ¤í¬ì— ì‚¬ìš©

### VideoBERT ì‚¬ì „ í•™ìŠµ

- **MLM(cloze íƒœìŠ¤í¬)ê³¼ ì–¸ì–´-ì‹œê°(linguistic-visual) ì •ë ¬ì´ë¼ëŠ” ìƒˆë¡œìš´ íƒœìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµ**

- ë°ì´í„°: êµìœ¡ìš© ë¹„ë””ì˜¤(e.g., ìš”ë¦¬ ë¹„ë””ì˜¤)

  - êµìœ¡ìì˜ ë§ê³¼ í•´ë‹¹ ì‹œê° ìë£Œ(ì˜ìƒ)ê°€ ì„œë¡œ ì¼ì¹˜í•´ ì–¸ì–´ì™€ ë¹„ë””ì˜¤ì˜ í‘œí˜„ì„ ë™ì‹œì— ë°°ìš°ëŠ”ë° ë„ì›€

- ë¹„ë””ì˜¤ì—ì„œ ì–¸ì–´ í† í°ê³¼ ì‹œê° í† í°ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµì— ì‚¬ìš©

  - ì–¸ì–´ í† í° ì¶”ì¶œ ë°©ë²•

    1. ë¹„ë””ì˜¤ì— ì‚¬ìš©ëœ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œ

    2. ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

       **ìë™ ìŒì„± ì¸ì‹**(automatic speech recognition) íˆ´í‚·ì„ í™œìš©
       _Cf. https://cloud.google.com/speech-to-text_

    3. í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ë©´ ì–¸ì–´ í† í° ìƒì„±

  - ì‹œê° í† í° ì¶”ì¶œ ë°©ë²•

    1. ë¹„ë””ì˜¤ì˜ ì´ë¯¸ì§€ í”„ë ˆì„ì„ 20fps(ì´ˆë‹¹ í”„ë ˆì„)ë¡œ ìƒ˜í”Œë§
    2. ì´ë¯¸ì§€ í”„ë ˆì„ì„ 1.5ì´ˆì˜ êµ¬ê°„ìœ¼ë¡œ ì‹œê° í† í°ë“¤ë¡œ ë³€í™˜

- ì…ë ¥ í† í°

  1. ì–¸ì–´ì™€ ì‹œê° í† í° ê²°í•©

     íŠ¹ìˆ˜ í† í° `[>]` ìœ¼ë¡œ ì–¸ì–´ ë° ì‹œê°ì  í† í° ê²°í•© 

  2. ì–¸ì–´ í† í°ì˜ ì‹œì‘ ë¶€ë¶„ì— `[CLS]` í† í° ì¶”ê°€, ì‹œê° í† í° ëì—ë§Œ `[SEP]` í† í° ì¶”ê°€

  3. ì–¸ì–´ ë° ì‹œê° í† í° ì¤‘ ëª‡ ê°€ì§€ í† í°ë“¤ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹

- **cloze íƒœìŠ¤í¬**

  - VideoBERTì—ì„œ ë°˜í™˜ëœ ë§ˆìŠ¤í¬ëœ í‘œí˜„ì„ ë¶„ë¥˜ê¸°(í”¼ë“œí¬ì›Œë“œ + ì†Œí”„íŠ¸ë§¥ìŠ¤)ì— ì…ë ¥í•˜ë©´, ë¶„ë¥˜ê¸°ëŠ” <u>ë§ˆìŠ¤í¬ëœ í† í° ì˜ˆì¸¡</u>

  ![cloze task](https://d3i71xaburhd42.cloudfront.net/c41a11c0e9b8b92b4faaf97749841170b760760a/4-Figure3-1.png)

- **ì–¸ì–´-ì‹œê° ì •ë ¬**

  - NSP íƒœìŠ¤í¬ì™€ ìœ ì‚¬í•œ ë¶„ë¥˜ íƒœìŠ¤í¬

  - ì–¸ì–´ì™€ ì‹œê° í† í°ì´ ì‹œê°„ì ìœ¼ë¡œ ì„œë¡œ ì •ë ¬ë˜ì–´ ìˆëŠ”ì§€ ì˜ˆì¸¡

    = <u>í…ìŠ¤íŠ¸(ì–¸ì–´ í† í°)ê°€ ë¹„ë””ì˜¤(ì‹œê°ì  í† í°)ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ì—¬ë¶€ ì˜ˆì¸¡</u>

  - `[CLS]` í† í°ì˜ í‘œí˜„ì„ ê°€ì ¸ì˜¨ ë‹¤ìŒ ì£¼ì–´ì§„ ì–¸ì–´ ë° ì‹œê° í† í°ì´ ì¼ì‹œì ìœ¼ë¡œ ì •ë ¬ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¶„ë¥˜ê¸°ì— ì…ë ¥

    _Cf. NSPëŠ” `[CLS]` í‘œí˜„ì„ ì´ìš©í•´ ì£¼ì–´ì§„ ë¬¸ì¥ ë‹¤ìŒì— ì¶œí˜„í•˜ëŠ” ë¬¸ì¥ì´ ë‹¤ìŒ ë¬¸ì¥ì¸ì§€ ì˜ˆì¸¡_

- **ìµœì¢… ì‚¬ì „ í•™ìŠµ ëª©í‘œ**

  - ì„¸ ê°€ì§€ ëª©í‘œ: í…ìŠ¤íŠ¸, ë¹„ë””ì˜¤, ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸

    - **í…ìŠ¤íŠ¸**

      ì–¸ì–´ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³  ë§ˆìŠ¤í¬ ì–¸ì–´ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì„œ ëª¨ë¸ì´ ì–¸ì–´ í‘œí˜„ì„ ë” ì˜ ì´í•´í•˜ë„ë¡ í•¨

    - **ë¹„ë””ì˜¤**

      ì‹œê° í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³  ë§ˆìŠ¤í¬ëœ ì‹œê° í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì„œ ëª¨ë¸ì´ ë¹„ë””ì˜¤ í‘œí˜„ì„ ë” ì˜ ì´í•´í•˜ë„ë¡ í•¨

    - **ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸**

      ì–¸ì–´ ë° ì‹œê°ì  í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³  ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì–¸ì–´ ë° ì‹œê° í† í°ì„ ì˜ˆì¸¡í•˜ê³ , ì–¸ì–´-ì‹œê° ì •ë ¬ì„ í•™ìŠµí•˜ê²Œ í•´ì„œ ëª¨ë¸ì´ ì–¸ì–´ì™€ ì‹œê° í† í° ê°„ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ê²Œ í•¨

  - ìµœì¢… ì‚¬ì „ í•™ìŠµ ëª©í‘œ: ì„¸ ê°€ì§€ ë°©ë²•ì„ ëª¨ë‘ í™œìš©í•œ ê°€ì¤‘ì¹˜ ì¡°í•©
  - 4ê°œì˜ TPUë¥¼ ì‚¬ìš©í•´ 2ì¼ ë™ì•ˆ 50ë§Œ ë²ˆ ë°˜ë³µí•´ ìµœì¢… ì‚¬ì „ í•™ìŠµ ëª©í‘œë¥¼ ìˆ˜í–‰í•˜ë©° í•™ìŠµ

### ë°ì´í„° ì†ŒìŠ¤ ë° ì „ì²˜ë¦¬

**ë°ì´í„°ì…‹**

- ìœ íŠœë¸Œì˜ êµìœ¡ìš© ë¹„ë””ì˜¤ ì‚¬ìš©

  - ìœ íŠœë¸Œ ë™ì˜ìƒ ì£¼ì„(annotation) ì‹œìŠ¤í…œì„ ì´ìš©í•´ ìš”ë¦¬ì™€ ê´€ë ¨ëœ ìœ íŠœë¸Œ ë™ì˜ìƒ ì¶”ì¶œ

    - 15ë¶„ ë¯¸ë§Œ ì˜ìƒ ê¸¸ì´, 31ë§Œ 2000ê°œì˜ ë™ì˜ìƒ

  - ìœ íŠœë¸Œ APIì—ì„œ ì œê³µí•˜ëŠ” ìë™ ìŒì„± ì¸ì‹ ë„êµ¬(ASR)ë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    - í…ìŠ¤íŠ¸ë¥¼ íƒ€ì„ ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ ê°–ê³ ì˜¤ê¸° ìœ„í•¨ (ë¹„ë””ì˜¤ì— ì‚¬ìš©ëœ ì–¸ì–´ì— ëŒ€í•œ ì •ë³´ë„ ë°˜í™˜)

    - 31ë§Œ 2000ê°œì˜ ë™ì˜ìƒ ì¤‘ 18ë§Œ ê°œì˜ ë™ì˜ìƒì—ë§Œ ASRì„ ì ìš©í•  ìˆ˜ ìˆì—ˆê³ , ê·¸ ì¤‘ ì˜ì–´ë¡œ ëœ ë™ì˜ìƒì€ 12ë§Œê°œë¡œ ì¶”ì •

      â¡ï¸ í…ìŠ¤íŠ¸ ë° ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ëª©í‘œë¥¼ ìœ„í•´ 12ë§Œ ê°œì˜ ë¹„ë””ì˜¤ë§Œ ì‚¬ìš©í•˜ê³ , ë¹„ë””ì˜¤ ëª©í‘œëŠ” 31ë§Œ 2000ê°œì˜ ë¹„ë””ì˜¤ ì‚¬ìš©

**ë¹„ë””ì˜¤ ë° ì–¸ì–´ ì „ì²˜ë¦¬**

- ì‹œê° í† í°
  1. ë¹„ë””ì˜¤ì˜ ì´ë¯¸ì§€ í”„ë ˆì„ì„ 20fps(ì´ˆë‹¹ í”„ë ˆì„)ë¡œ ìƒ˜í”Œë§
  2. ì´ë¯¸ì§€ í”„ë ˆì„ì„ 1.5ì´ˆì˜ êµ¬ê°„ìœ¼ë¡œ ì‹œê° í† í°ë“¤ë¡œ ë³€í™˜ (= 30-frame clip ìƒì„±)
  3. ê° 30-frame clipì— ì‚¬ì „ í•™ìŠµëœ ë¹„ë””ì˜¤ ì»¨ë³¼ë£¨ì…”ë„ ë‰´ëŸ´ë„·ì„ ì ìš©í•´ íŠ¹ì§• ì¶”ì¶œ
  3. ê³„ì¸µì  k-í‰ê·  ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ ì‹œê° íŠ¹ì§• í† í°í™”
- ì–¸ì–´ í† í°
  1. ìƒìš© LSTM ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•´ ê° ASR ë‹¨ì–´ ì‹œí€€ìŠ¤ì— êµ¬ë‘ì ì„ ì¶”ê°€í•˜ì—¬ ë‹¨ì–´ ìŠ¤íŠ¸ë¦¼ì„ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ”
  2. ê° ë¬¸ì¥ì— ëŒ€í•´ BERTì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹ì„ ë”°ë¥´ë©°, í…ìŠ¤íŠ¸ í† í°í™” 

> ğŸ’¡ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ë‰˜ëŠ” ì–¸ì–´ì™€ ë‹¬ë¦¬, ë¹„ë””ì˜¤ëŠ” ì–´ë–»ê²Œ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë‚˜ëˆŒê¹Œ?
>
> - íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
>   - ASR ë¬¸ì¥ì´ ì¡´ì¬í•  ê²½ìš°, ë¬¸ì¥ì˜ ì‹œì‘ ë° ì¢…ë£Œ timestamp ì‚¬ì´ì— í•´ë‹¹í•˜ëŠ” ë¹„ë””ì˜¤ í† í°ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì·¨ê¸‰
>   - ASR ë¬¸ì¥ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°, í•˜ë‚˜ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ 16ê°œì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰

### VideoBERTì˜ ì‘ìš©

ì‚¬ì „í•™ìŠµëœ VideoBERT ëª¨ë¸ì„ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ <u>ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬</u>ì— ë§ì¶° íŒŒì¸íŠœë‹

- ì‹œê° í† í°ì„ ì…ë ¥í•´ ìƒìœ„ 3ê°œì˜ ë‹¤ìŒ ì‹œê° í† í° ì˜ˆì¸¡
- í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹í•˜ëŠ” ë¹„ë””ì˜¤ ìƒì„±
- ë¹„ë””ì˜¤ì— ìë§‰ ìƒì„±

## Transformers in vision

ğŸ“„ [Transformers in Vision: A Survey](https://arxiv.org/pdf/2101.01169.pdf)

- Computer Vision ë¶„ì•¼ì—ì„œ Transformerê°€ í™œìš©ëœ ì—°êµ¬ë“¤ ì •ë¦¬í•œ survey paper

### Background

RNN to Transformer in NLP â‡¨ CNNì— self-attention ì ìš© â‡¨ Transformer ëª¨ë¸ ìì²´ë¥¼ CV íƒœìŠ¤í¬ì— ì‚¬ìš©

### Task

![TABLE 1: A summary of key design choices adopted in different variants of transformers for a representative set of computer vision applications.](https://d3i71xaburhd42.cloudfront.net/3a906b77fa218adc171fecb28bb81c24c14dcc7b/21-Table1-1.png)

***Table 1 from Transformers in Vision: A survey***

*A summary of key design choices adopted in different variants of transformers for a representative set of computer vision applications. The main changes relate to specific loss function choices, architectural modifications, different position embeddings and variations in input data modalities.*

### Model

#### Transformers for Multi-Modal Tasks 

*Multi-modal learning: ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…, ë°ì´í„° í˜•íƒœ, ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ê°–ëŠ” ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” í•™ìŠµë²•*

- Transformer ëª¨ë¸ì€ <u>vision-language íƒœìŠ¤í¬</u>ì—ë„ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©
  - visual question answering (VQA)
  - visual commonsense reasoning (VCR)
  - cross-modal retrieval
  - image captioning

![Fig. 12: An overview of Transformer models used for multi-modal tasks in computer vision](https://d3i71xaburhd42.cloudfront.net/3a906b77fa218adc171fecb28bb81c24c14dcc7b/14-Figure12-1.png)

*An overview of Transformer models used for multi-modal tasks in computer vision*

#### Vision Transformer (ViT) 

ğŸ“„ [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929.pdf)

![Fig. 6: An overview of Vision Transformer (on the left) and the details of Transformer encoder (on the right)](https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-14_at_2.26.57_PM_WBwCIco.png)

- CNN êµ¬ì¡°ì˜€ë˜ computer vision ë¬¸ì œë¥¼ Transformer êµ¬ì¡°ë¡œ ëŒ€ì²´
  - Transformer êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œ Architectureê°€ ìˆ˜ ë§ì€ SOTAë¥¼ ì°ê³  ìˆìœ¼ë©°, **ViT ë…¼ë¬¸ì´ ê·¸ ì‹œì‘ì **
  - Transformer êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ image classificationì„ ìˆ˜í–‰í•œ ë°©ë²•ë¡ 
  - ë” ë§ì€ ë°ì´í„°ë¥¼ ë” ì ì€ ë¹„ìš©ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµ

- êµ¬ì¡°

  - Transformer encoder ì‚¬ìš©
  - í•œ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ patchë¡œ ë¶„í•  (patchë¥¼ ë‹¨ì–´ê°™ì´ ì·¨ê¸‰)
  - patch, classification token, position embeddingì„ ì…ë ¥í•˜ì—¬ ìµœì¢… classification ê²°ê³¼ ìƒì„±

  > CNN vs Transformer
  >
  > - Layer
  >
  >   - CNN: ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ê¸° ìœ„í•´ì„œëŠ” ëª‡ ê°œì˜ layer í†µê³¼
  >   - Transformer: í•˜ë‚˜ì˜ layerë¡œ ì „ì²´ ì´ë¯¸ì§€ ì •ë³´ í†µí•© ê°€ëŠ¥
  >
  > - Inductive bias
  >
  >   _ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ëª¨ë¸ì— ì‚¬ì „ì ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ê°€ì •_
  >
  >   - CNN: 2ì°¨ì›ì˜ ì§€ì—­ì ì¸ íŠ¹ì„± ìœ ì§€, í•™ìŠµ í›„ weight ê³ ì •
  >
  >     â†’ ì¸ì ‘í•œ í”½ì…€ ê°„ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ëŠ” íŠ¹ì§•ì„ ì‚´ë ¤ inductive biasê°€ ì ì ˆí•˜ê²Œ ë§Œë“¤ì–´ì§ìœ¼ë¡œì¨ ì´ë¯¸ì§€ íŠ¹ì§• íš¨ê³¼ì  ì¶”ì¶œ
  >
  >   - Transformer: 1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“  í›„ self attention (2ì°¨ì›ì˜ ì§€ì—­ì ì¸ ì •ë³´ ìœ ì§€ x), weightê°€ inputì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë³€í•¨
  >
  >     â†’ inductive biasê°€ ì ê³ , ëª¨ë¸ì˜ ììœ ë„ê°€ ë†’ì•„ ë°ì´í„°ë¡œë¶€í„° ë” ë§ì€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

- í•œê³„

  - í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš° CNN ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ ê°ì†Œ (âˆµ inductive bias â†“)

    â¡ï¸ ëŒ€ìš©ëŸ‰ì˜ í•™ìŠµ ìì›ê³¼ ë°ì´í„°ê°€ í•„ìš”

#### Data efficient image Transformer (DeiT)

ğŸ“„ [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)

- ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•œ ViT í•œê³„ ê·¹ë³µ
  - Knowledge Distilation
  - Data Augmentation

## ì°¸ê³  ìë£Œ

#### ê°œë…

- [Multimodal Deep Learning](https://towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4)

#### ì„¸ë¯¸ë‚˜

- [DMQA Transformer in Computer Vision](http://dmqm.korea.ac.kr/activity/seminar/316)

#### ë…¼ë¬¸

- [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/pdf/1904.01766.pdf)
- [Transformers in Vision: A Survey](https://arxiv.org/pdf/2101.01169.pdf)
  - **ë¦¬ë·°**
    - [Transformers in Visionï¼š A Survey [1] Transformer ì†Œê°œ & Transformers for Image Recognition](https://hoya012.github.io/blog/Vision-Transformer-1/)
- [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929.pdf)
  - **ë¦¬ë·°**
    - https://engineer-mole.tistory.com/133
    - https://kmhana.tistory.com/27

#### ì±…

- [êµ¬ê¸€ BERTì˜ ì •ì„](http://www.yes24.com/Product/Goods/104491152)
